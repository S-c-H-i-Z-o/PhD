let\cleardoublepage\clearpage
\chapter{Introduction}

La Business Intelligence (BI) a commme usage, la création de nouvelles perspectives pour les entreprises en transformant les données en information, en vue d'offrir une aide a la decision qui pourrait etre
utilisee par les décideurs et les dirigeants afin de stimuler l'evolution de l'entreprise. Un aspect clé de l'extraction de l'information à partir des données est d'en avoir une compréhension commune et partagéee, ceci est également appelé La Sémantique

La BI classique et même les nouveaux outils de visualisation Agile consacrent une grande partie de leurs fonctionnalités sur des visualisations attrayantes et uniques.
Mais la préparation des données pour ces visualisations cependant reste la tâche la plus difficile dans la plupart des projets de BI, qu'il soit petit ou grand. 
 
Le but ultime de la BI est de faciliter la prise de décision efficace tout en simplifiant la tache des IT. 
Traditionnellement, les approches de la BI ont été contrôlées par une version centralisée de la vérité avec une barriere entre les IT et les derigents de l'entreprise. 
Le libre-service de d'aprovisionnement de données vise à éliminer cette bariere grâce à la decoouverte et la acquisition des jeux de données intuitives et des techiniques d'integration intuitives 
pour l'utilisateur.

\section{Contexte et Motivation} \label{section:motivation}

Les entreprises utilisent une multitude de systèmes d'information hétérogènes dans leurs activités commerciales telles que les Progiciels de Gestion Intégré (Enterprise esource planning ou ERP), 
Les systemes de gestion de la relation client (Customer Relationship Management ou CRM) et Les systemes de la Gestion de la chaîne logistique (Supply Chain Management ou SCM). 
Un environnement IT, d'entreprise, distribué contient plusieurs systèmes utilisant des technologies et des normes de données~\cite{Mihindukulasooriya:COLD:13} différentes. 
En plus de cette hétérogénéité, la quantité d'informations dans les bases de données de l'entreprise et data store en ligne augmente chaque année de façon exponentielle . 
Le Big Data en enterprise n'est pas seulement une question de volume, mais aussi bien dans les formats de fichiers associés. De plus l'information est souvent stockée dans des formats de fichier inconnus non structurés.

L'intégration des données est difficile car elle nécessite la combinaison de données stockees dans différentes sources, et fournir à l'utilisateur une vue unifiée de ces données~\cite{Lenzerini:SIGMOD:02}.
Dans les grandes entreprises, cela a cout considerable en temps et en ressources. Plusieures approches ont été proposées pour résoudre ce défi d'intégration. Ces approches ont été principalement basées
sur XML comme format de représentation de données, les services Web pour fournir les protocoles d'échange de données et la Service-Oriented Architecture (SOA) comme une approche globale pour les 
systèmes ayant une architecture et une communication distribuées.
Cependant, il a été constaté que ces technologies ne sont pas suffisantes pour résoudre les problèmes d'intégration de donnees dans les grandes entreprises~\cite{Frischmuth:ISWC:13,Frischmuth:SemWebJorunal:12}. 
Récemment, des approches d'intégration de données basés sur les ontologie ont été suggérées où les ontologies sont utilisés  pour décrire les données, les requêtes et les correspondances entre elles~\cite{Wache:IJCAI:01}. 
Une approche légèrement différente consiste à l'utilisation du paradigme Web des Donnees~\cite{Bizer:IJSWIS:09} pour l'intégration de données d'entreprise. Des entreprises comme Google et Microsoft 
n'utilisent pas le paradigme de Web de Donnes seulement pour l'intégration de données pour leurs systèmes d'information, mais elles visent également à mettre en place les bases de connaissances de 
l'entreprise (comme le Google Knowledge Graph alimenté en partie par Freebase \ footnote {\ url {http: // freebase .com}}) qui agissent comme un point de cristallisation pour leurs données structurées .

Les données deviennent plus utile quand elles sont ouvertes, largement disponibles dans des formats partageables. La qualité et la quantité de connaissance structurée disponible aujourd'hui sur le web
permet désormais aux entreprises d'exploiter cette énorme quantité de données publiques et de l'intégrer dans leurs futurs systèmes de gestion d'information d'entreprise. Un exemple de ces données
externe est le Linked Open Data (LOD) cloud. A partir de 12 ensembles de données cataloguées en 2007, il a grandi aujourd'hui pour près de 1000 jeux de données contenant plus de 82 milliards de triplets\footnote{\url{http://datahub.io/dataset?tags=lod}}~\cite{Bizer:IJSWIS:09}. 
Les données sont publiées par les secteurs public et privé, et couvre un ensemble diversifié de domaines, des sciences de la vie aux médias en arrivant aux données gouvernementale. Le LOD cloud est potentiellement
 une mine d'or pour les organisations et les individus qui cherchent à tirer parti de sources de données externes pour faire des décisions d'affaires plus efficaces~\cite{Boyd:Article:11}. 
 Ces données externes peuvent être accessibles via des portails de données publiques comme \texttt {datahub.io} et \texttt {publicdata.eu} ou privés comme \texttt{quandl.com} et \texttt{enigma.io}.
 L'analyse de ce nouveau type de données dans le contexte des données d'entreprise existant devrait leur apporter de nouvelles ou plus précises analyses commerciales et permettre une meilleure 
 reconnaissance des opportunités de marché~\cite{LaValle:MIT:11}.

\section{Scénario d'utilisation}\label{section:scenario}

Pour permettre une intégration efficace et à grande échelle des données, il ya quelques efforts nécessaires de plusieurs côtés. Dans cette thèse, nous abordons les enjeux et les défis du point de vue
 de deux personnages:

\begin{itemize}
	\item \textbf{Analyste de données:} Un analyste de données est un professionnel expérimenté qui est en mesure de recueillir et d'acquérir des données provenant de multiples sources de données,
	filtrer et nettoyer les données, interpréter et analyser les résultats et fournir des rapports de progression.
	\item \textbf{Administrateur du portail de données:} Un administrateur du portail de données surveille le status globale d'un portail. Il supervise la création des utilisateurs, des organisations 
	et des ensembles de données. Les administrateurs essayent d'assurer un niveau de qualité de certaines données en vérifiant en permanence la presence de spam et l'amélioration manuelle 
	de la descriptions et l'annotations des ensembles de données
\end{itemize}

Tout au long de cette thèse, nous allons présenter un scénario de cas d'utilisation impliquant les deux personnages afin d'illustrer les défis et les solutions que nous fournissons.

Dans notre scénario, \textbf{Dan} est un analyste de données qui collabore avec le ministère des Transports en France. Son outil prefere pour les calculs, la manipulation et la visualisation
de données est SAP Lumira\footnote{\url{http://saplumira.com/}}, qui est un outil de visualisation de données en libre-service qui facilite, l'import des données provenant de plusieurs sources,
l'analyse BI visuelle à l'aide de tableaux de bord intuitifs, des cartes interactives, des graphiques, et des infographies. Dan reçoit une note de sa direction pour créer un rapport comparant
le nombre d'accidents de voiture qui ont eu lieu cette année en France, à son homologue du Royaume-Uni (UK). En outre, il est demandé de mettre en évidence les accidents liés à la consommation 
illégale d'alcool dans les deux pays.

Après avoir examiné les dossiers du ministère, Dan est en mesure de rassembler les données nécessaires pour créer son rapport pour la partie française. Dan publie également une demande officielle 
au ministère des Transports au Royaume-Uni pour collecter les données nécessaires. Cependant, Dan sait que le processus prend beaucoup de temps et la direction veut avoir le rapport dans quelques jours. 
Dan est familier avec le mouvement Open Data et commence se balader à travers différents les portails de recherche de données au Royaume-Uni.

\textbf{Paul} est un administrateur du portail de données pour le \texttt{data.gov.uk}. Il supervise en permanence les processus d'acquisition, de préparation et de publication des ensembles de données. 
Paul essaie toujours de veiller à ce que les données publiées soient de haute qualité et qui contiennent des métadonnées suffisantes pour faciliter la recherche et de la découverte. 
Paul reçoit souvent des plaintes par rapport a des ensembles de données imprecis ou des spams. Il supprime et corrige manuellement les erreurs tout en gardant les canaux de communication ouverts
avec les services de données de publication.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Research Challenges  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Défis de la Recherche} \label{section:challenges}

Dans le scénario présenté ci-dessus, les deux intervenants, les éditeurs (administrateurs du portail de données) et les utilisateurs (analystes de données) ont besoin de solutions pragmatiques qui les aident dans leurs
 tâches. Pour permettre cela, il ya quelques questions de recherche complexes qui doivent être abordées. Ces défis sont organisés en trois grandes catégories comme suit:

\subsection{Intégration et enrichissement de Données}

\begin{itemize}
	\item Les Sources de données d'entreprise hétérogènes posent des défis énormes. Ils ont fondamentalement différents; formats de fichiers, protocoles d'accès ou langages de requête. 
	Ils possèdent leur propre modèle de données avec différentes façons de représentation et de stockage de données. Ayant toutes ces sources de donnees peut ramener a avoir du bruit (des donnees dupliquees ou incompatibles),
	des donnees incertaines ou des donnees sémantiquement similaires mais pourtant différentes.\textbf{Paul} a besoin d'outils tres perfomants pour mapper et organiser les données afin d'avoir une vue unifiée pour ces structures de
	données hétérogènes et complexes.
	\item Rajouter des métadonnées et des informations sémantiques aux instances peut être délicat. Une entité est généralement non associée à un type générique unique dans la base de connaissances,
	mais plutôt à un ensemble de types spécifiques qui peuvent être pertinents ou pas compte tenu du contexte. \textbf{Paul} a donc un defi qui consiste à trouver le type de d'entité la plus pertinent dans un 
	contexte donné.
	\item Les entités jouent un rôle clé dans les bases de connaissances en général et dans le Web de Données en particulier. Les entités comme ceux de DBpedia, sont généralement décrits avec beaucoup de
	propriétés. Cependant, il est difficile pour \textbf{Dan} d'évaluer celles qui sont les plus ``importantes'' pour des tâches particulières, telles l'augmentation des données et de visualiser
	les faits principaux d'une entité.
	\item Les réseaux sociaux ne rassemblent pas seulement les utilisateurs d'Internet en groupes ayant d'intérêts communs, ils aident aussi les gens suivre les dernieres infos, contribuer aux débats 
	en ligne ou apprendre des autres. Ils sont en train de transformer l'utilisation du Web en termes de point d'entrée intiales des utilisateurs, de la recherche, de la navigation et l'achat en ligne.
	Cependant, l'intégration des informations de ces réseaux sociaux peut être difficile à \textbf{Paul} en raison de la grande quantité de données disponibles ce qui rend difficile à repérer ce qui est pertinent en temps opportun.
\end{itemize}

\subsection{Maintenance et Découverte de Données}

\begin{itemize}
	\item Même si les ensembles de données les plus populaires comme DBPedia\footnote{\url{http://dbpedia.org}} et Freebase sont bien connus et largement utilisés, il existe d'autres ensembles de données utiles caché 
	qui ne sont pas utilisés. En effet, ces ensembles de données peuvent être utiles pour les domaines spécialisés, mais sans avoir une visibilite sur leurs sujets, il est difficile pour les analystes de données comme \textbf{Dan} de les trouver~\cite{Lalithsena:WI:13}.
	\item La de données qui ne cesse d'augmenter nécessite des métadonnées riches pour atteindre son vrai potentiel. Ces métadonnées permet la découverte de données, la compréhension, l'intégration et la
	maintenance. Malgré les différents modèles et les différents vocabulaires décrivant les ensembles de données des métadonnées, la capacité d'avoir un aperçu de l'ensemble de données en inspectant ses métadonnées peut être limité. 
	Par example, \textbf{Dan} a des difficultés à trouver des ensembles de données ayant une couverture géographique spécifique, car cette information n'existe pas presque dans tous les profils de jeux de données examinés.
	\item Les utilisateurs, les organisations et les gouvernements sont habilités à publier des ensembles de données. Toutefois, les administrateurs du portail de données comme \textbf{Paul} ont besoin 
	de vérifier manuellement et en permanence les portails pour détecter les spams et pour maintenir des données de haute qualité.
\end{itemize}

\subsection{Qualité de Données}

Le Web de Donnees se compose de l'information structurée soutenue par des modèles, des ontologies et des vocabulaires, et contient les liens et les points d'entrees des requestes. Cela rend l'assurance de la qualité des
 données comme un défi. Malgré le fait que la qualité de la Linked Open Data est une tendance et le sujet est d'actualite, il n'y a pas beaucoup d'efforts pour essayer de normaliser, de suivre et de formaliser les 
 des certificats ou des scores qui aideront les consommateurs de données dans leurs tâches d'intégration. Les administrateurs de portail de données comme \textbf{Paul} ont besoin d'avoir
 une vision globale de la qualité de leurs portails et que ils voulent bien intégrer ces paramètres dans les profils d'ensembles de données existants. D'autre part, les analystes de données et les 
 utilisateurs comme \textbf{Dan} veulent savoir à l'avance si l'ensemble de données qu'ils ont a certain degré de qualité pour être utilisé dans leurs rapports.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Thesis Contributions  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Contributions de Thèse} \label{section:contribution}

Dans cette thèse, nous proposons un cadre pour permettre la fourniture de données en libre-service pour les sources de données internes et externes à l'entreprise. Le cadre contribue aux trois
 principaux défis décrits ci-dessus. En résumé, les principales contributions de ce travail sont les suivants:

\begin{adjustwidth}{-.4in}{-.4in}
	\begin{figure}[!ht]
	 \centering
	 \includegraphics[scale=0.4]{figures/architecutre_diagram.png}
	 \caption{Schéma de l'architecture des données pour permettre l'approvisionnement en libre-service}
	 \label{fig:architecutre_diagram}
	\end{figure}
\end{adjustwidth}

\subsection{Contributions sur Maintenance et Découverte de Données}

En ce qui concerne cet aspect de notre recherche, nous avons accompli les tâches suivantes:
\begin{itemize}
	\item Nous avons interrogé le paysage de différents modèles et des vocabulaires qui décrivent des ensembles de données sur le web. Depuis la création d'un vocabulaire commun ou le modèle est la clé
	de la communication, nous avons identifié le besoin d'un modèle de métadonnées de jeu de données harmonisée contenant suffisamment d'informations afin que les consommateurs peuvent facilement
	comprendre et ensembles de données de processus. Premièrement, nous avons mis en place un ensemble de correspondances entre chacune des propriétés des modèles étudiés. Ceci a conduit à la conception 
	de HDL, un modèle de données harmonisée, qui prend le meilleur sur ces modèles et les étend à assurer une couverture complète de métadonnées pour permettre la découverte de données, l'exploration et
	la réutilisation.
	\item Nous avons analysé le paysage des outils de profilage des ensembles de données et découvert diverses lacunes. En conséquence, nous avons proposé Roomba, un cadre évolutif pour extraire 
	automatique, la validation, la création et de générer des profils d'ensembles de données liées descriptives. Roomba applique plusieurs techniques afin de vérifier la validité des métadonnées 
	fournies et pour générer des informations descriptives et statistiques pour un ensemble de données particulier ou pour un portail de données entière.
\end{itemize}

\subsection{Contributions sur la Qualité de Données}
Concernant nos contributions sur l'évaluation de la qualité Linked Data, nous avons accompli les tâches suivantes:
\begin{itemize}
	\item Nous avons proposé un cadre d'évaluation de la qualité des données liées concentrant sur les mesures objectives des données. Nous avons identifié un total de 64 indicateurs de qualité qui 
	ont été mappés lorsque approprié pour quatre catégories principales (entité, DataSet liens, modèles) correspondant aux principes de la publication de données de base lié.
	\item Sur l'arpentage du paysage des outils de qualité de données, nous avons remarqué un manque dans les outils automatiques pour vérifier les paramètres de qualité de l'ensemble de données
	proposées dans notre cadre. En conséquence, nous avons étendu Roomba pour effectuer une série de contrôles de qualité des données sur les ensembles de données liés. Notre extension couvre la 
	plupart des indicateurs de qualité proposés avec un accent sur l'exhaustivité, l'exactitude, la provenance et les licences.
\end{itemize}

\subsection{Contributions sur l'intégration et l'enrichissement de Données}

En ce qui concerne cet aspect de notre recherche, nous avons accompli les tâches suivantes:
\begin{itemize}
	\item Nous avons créé un cadre appelé RUBIX qui permet de données d'entreprise potentiellement bruyants brassage-up et des données externes. Le cadre exploite des bases de connaissances de référence
	pour annoter des données avec un ensemble de concepts sémantiques (métadonnées). Un des avantages de ces métadonnées est d'améliorer le processus d'appariement des sources de données hétérogènes au
	sein d'une entreprise.
	\item Les métadonnées attachée par RUBIX peut encore être utilisé pour enrichir les ensembles de données existants. Toutefois, les concepts sont souvent représentés avec un grand ensemble de 
	propriétés. Pour mieux recommander le haut ``importants'' propriétés d'un concept, nous inversés ingénieur les choix faits par Google lors de la création des panneaux de graphes de connaissances
	et présentés ces choix en utilisant explicitement le vocabulaire de Fresnel, de sorte que toute application peut lire ce fichier de configuration pour décider qui propriétés d'une entité qui est 
	intéressant à enrichir.
	\item Agrégation nouvelles sociale pertinente est pas une tâche facile. Nous fournissons une Application Programming Interface (API) qui permet l'agrégation de nouvelles sociale sémantique appelé
	SNARC. Nous avons conçu un exemple d'application frontend tirant parti des capacités de SNARK pour permettre aux utilisateurs de découvrir instantanément les nouvelles sociale pertinente.
\end{itemize}