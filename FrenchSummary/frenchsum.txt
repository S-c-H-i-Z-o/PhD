let\cleardoublepage\clearpage
\chapter{Introduction}

La Business Intelligence (BI) à comme usage, la création de nouvelles perspectives pour les entreprises en transformant les données en informations, en vue d'offrir une aide à la décision qui pourrait être
utilisée par les décideurs et les dirigeants afin de stimuler l'évolution de l'entreprise. Un aspect clé de l'extraction de l'information à partir des données est d'en avoir une compréhension commune et partagée, ceci est également appelé La Sémantique

La BI classique et même les nouveaux outils de visualisation Agile consacrent une grande partie de leurs fonctionnalités sur des visualisations attrayantes et uniques.
Mais la préparation des données pour ces visualisations toutefois reste la tâche la plus difficile dans la plupart des projets de la BI, qu'il soit petit ou grand.
 
Le but ultime de la BI est de faciliter la prise de décision efficace tout en simplifiant la tâche des IT. 
Traditionnellement, les approches de la BI ont été contrôlées par une version centralisée de la vérité avec une barrière entre les IT et les dirigeants de l'entreprise. 
Le libre-service de d'approvisionnement de données vise à éliminer cette barrière grâce à la découverte et l’acquisition des jeux de données intuitives et des techniques d'intégration intuitives 
pour l'utilisateur.

\section{Contexte et Motivation} \label{section:motivation}

Les entreprises utilisent une multitude de systèmes d'information hétérogènes dans leurs activités commerciales telles que les Progiciels de Gestion Intégré (Enterprise source planning ou ERP), 
Les systèmes de gestion de la relation client (Customer Relationship Management ou CRM) et Les systèmes de la Gestion de la chaîne logistique (Supply Chain Management ou SCM). 
Un environnement IT, d'entreprise, distribué contient plusieurs systèmes utilisant des technologies et des normes de données~\cite{Mihindukulasooriya:COLD:13} différentes. 
En plus de cette hétérogénéité, la quantité d'informations dans les bases de données de l'entreprise et dans les data store en ligne augmente chaque année de façon exponentielle . 
Le Big Data en enterprise n'est pas seulement une question de volume, mais aussi une question des formats de fichiers associés. De plus l'information est souvent stockée dans des formats de fichier inconnus et non structurés.

L'intégration des données est difficile car elle nécessite la combinaison de données stockées dans des différentes sources, et fournir à l'utilisateur une vue unifiée de ces données~\cite{Lenzerini:SIGMOD:02}.
Dans les grandes entreprises, cela a un cout considérable en temps et en ressources. Plusieurs approches ont été proposées pour résoudre ce défi d'intégration. Ces approches ont été principalement basées
sur XML comme format de représentation de données, les services Web pour fournir les protocoles d'échange de données et la Service-Oriented Architecture (SOA) comme une approche globale pour les 
systèmes ayant une architecture et une communication distribuées.
Cependant, il a été constaté que ces technologies ne sont pas suffisantes pour résoudre les problèmes d'intégration de données dans les grandes entreprises~\cite{Frischmuth:ISWC:13,Frischmuth:SemWebJorunal:12}. 
Récemment, des approches d'intégration de données basés sur les ontologie ont été suggérées où les ontologies sont utilisées  pour décrire les données, les requêtes et les correspondances entre elles~\cite{Wache:IJCAI:01}. 
Une approche légèrement différente consiste à l'utilisation du paradigme du Web des Donnees~\cite{Bizer:IJSWIS:09} pour l'intégration de données d'entreprise. Des entreprises comme Google et Microsoft 
n'utilisent pas le paradigme de Web de Donnes seulement pour l'intégration de données pour leurs systèmes d'information, mais elles visent également à mettre en place les bases de connaissances de 
l'entreprise (comme le Google Knowledge Graph alimenté en partie par Freebase \ footnote {\ url {http: // freebase .com}}) qui agissent comme un point repère pour leurs données structurées.

Les données deviennent plus utiles quand elles sont ouvertes, largement disponibles dans des formats partageables. La qualité et la quantité de connaissance structurée disponible aujourd'hui sur le web
permet désormais aux entreprises d'exploiter cette énorme quantité de données publiques et de l'intégrer dans leurs futurs systèmes de gestion d'information d'entreprise. Un exemple de ces données
externes est le Linked Open Data (LOD) cloud. A partir de 12 ensembles de données catalogués en 2007, il a grandi aujourd'hui pour près de 1000 jeux de données contenant plus de 82 milliards de triplets\footnote{\url{http://datahub.io/dataset?tags=lod}}~\cite{Bizer:IJSWIS:09}. 
Les données sont publiées par les secteurs public et privé, et couvre un ensemble diversifié de domaines, des sciences de la vie aux médias en arrivant aux données gouvernementale. Le LOD cloud est potentiellement
 une mine d'or pour les organisations et les individus qui cherchent à tirer parti de sources de données externes pour prendre des décisions d'affaires plus efficaces~\cite{Boyd:Article:11}. 
 Ces données externes peuvent être accessibles via des portails de données publiques comme \texttt {datahub.io} et \texttt {publicdata.eu} ou privés comme \texttt{quandl.com} et \texttt{enigma.io}.
 L'analyse de ce nouveau type de données dans le contexte des données d'entreprise existant devrait leur apporter de nouvelles ou plus précises analyses commerciales et permettre une meilleure 
 reconnaissance des opportunités de marché~\cite{LaValle:MIT:11}.


\section{Scénario d'utilisation}\label{section:scenario}

Pour permettre une intégration efficace et à grande échelle des données, il y a quelques efforts nécessaires de plusieurs côtés. Dans cette thèse, nous abordons les enjeux et les défis du point de vue
 de ces deux personnages:

\begin{itemize}
	\item \textbf{Analyste de données:} Un analyste de données est un professionnel expérimenté qui est en mesure de recueillir et d'acquérir des données provenant de multiples sources de données,
	filtrer et nettoyer les données, interpréter et analyser les résultats et fournir des rapports de progression.
	\item \textbf{Administrateur du portail de données:} Un administrateur du portail de données surveille le statu globale d'un portail. Il supervise la création des utilisateurs, des organisations 
	et des ensembles de données. Les administrateurs essayent d'assurer un niveau de qualité de certaines données en vérifiant en permanence la présence de spam et l'amélioration manuelle 
	de la descriptions et l'annotations des ensembles de données
\end{itemize}

Tout au long de cette thèse, nous allons présenter un scénario de cas d'utilisation impliquant les deux personnages afin d'illustrer les défis et les solutions que nous fournissons.

Dans notre scénario, \textbf{Dan} est un analyste de données qui collabore avec le ministère des Transports en France. Son outil préféré pour les calculs, la manipulation et la visualisation
de données est SAP Lumira\footnote{\url{http://saplumira.com/}}, qui est un outil de visualisation de données en libre-service qui facilite, l'import des données provenant de plusieurs sources,
l'analyse BI visuelle à l'aide de tableaux de bord intuitifs, des cartes interactives, des graphiques, et des infographies. Dan reçoit une note de sa direction pour créer un rapport comparant
le nombre d'accidents de voiture qui ont eu lieu cette année en France, à son homologue du Royaume-Uni (UK). En outre, il est demandé de mettre en évidence les accidents liés à la consommation 
illégale d'alcool dans les deux pays.

Après avoir examiné les dossiers du ministère, Dan est en mesure de rassembler les données nécessaires pour créer son rapport pour la partie française. Dan publie également une demande officielle 
au ministère des Transports au Royaume-Uni pour collecter les données nécessaires. Cependant, Dan sait que le processus prend beaucoup de temps et la direction veut avoir le rapport dans quelques jours. 
Dan est familier avec le mouvement Open Data et commence à se balader à travers les différents portails de recherche de données au Royaume-Uni.

\textbf{Paul} est un administrateur du portail de données pour le \texttt{data.gov.uk}. Il supervise en permanence les processus d'acquisition, de préparation et de publication des ensembles de données. 
Paul essaie toujours de veiller à ce que les données publiées soient de haute qualité et qui contiennent des métadonnées suffisantes pour faciliter la recherche et la découverte. 
Paul reçoit souvent des plaintes par rapport à des ensembles de données imprécis ou contenant des spams. Il supprime et corrige manuellement les erreurs tout en gardant les canaux de communication ouverts
avec les services de données de publication.


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Research Challenges  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Défis de la Recherche} \label{section:challenges}

Dans le scénario présenté ci-dessus, les deux intervenants, les éditeurs (administrateurs du portail de données) et les utilisateurs (analystes de données) ont besoin de solutions pragmatiques qui les aident dans leurs
 tâches. Pour permettre cela, il ya quelques questions de recherche complexes qui doivent être abordées. Ces défis sont organisés en trois grandes catégories comme suit:

\subsection{Intégration et enrichissement de Données}

\begin{itemize}
	\item Les Sources de données d'entreprise hétérogènes posent des défis énormes. Ils ont fondamentalement différents; formats de fichiers, protocoles d'accès ou langages de requête. 
	Ils possèdent leur propre modèle de données avec différentes façons de représentation et de stockage de données. Ayant toutes ces sources de donnees peut ramener a avoir du bruit (des donnees dupliquees ou incompatibles),
	des donnees incertaines ou des donnees sémantiquement similaires mais pourtant différentes.\textbf{Paul} a besoin d'outils tres perfomants pour mapper et organiser les données afin d'avoir une vue unifiée pour ces structures de
	données hétérogènes et complexes.
	\item Rajouter des métadonnées et des informations sémantiques aux instances peut être délicat. Une entité est généralement non associée à un type générique unique dans la base de connaissances,
	mais plutôt à un ensemble de types spécifiques qui peuvent être pertinents ou pas compte tenu du contexte. \textbf{Paul} a donc un defi qui consiste à trouver le type de d'entité la plus pertinent dans un 
	contexte donné.
	\item Les entités jouent un rôle clé dans les bases de connaissances en général et dans le Web de Données en particulier. Les entités comme ceux de DBpedia, sont généralement décrits avec beaucoup de
	propriétés. Cependant, il est difficile pour \textbf{Dan} d'évaluer celles qui sont les plus ``importantes'' pour des tâches particulières, telles l'augmentation des données et de visualiser
	les faits principaux d'une entité.
	\item Les réseaux sociaux ne rassemblent pas seulement les utilisateurs d'Internet en groupes ayant d'intérêts communs, ils aident aussi les gens suivre les dernieres infos, contribuer aux débats 
	en ligne ou apprendre des autres. Ils sont en train de transformer l'utilisation du Web en termes de point d'entrée intiales des utilisateurs, de la recherche, de la navigation et l'achat en ligne.
	Cependant, l'intégration des informations de ces réseaux sociaux peut être difficile à \textbf{Paul} en raison de la grande quantité de données disponibles ce qui rend difficile à repérer ce qui est pertinent en temps opportun.
\end{itemize}

\subsection{Maintenance et Découverte de Données}

\begin{itemize}
	\item Même si les ensembles de données les plus populaires comme DBPedia\footnote{\url{http://dbpedia.org}} et Freebase sont bien connus et largement utilisés, il existe d'autres ensembles de données utiles caché 
	qui ne sont pas utilisés. En effet, ces ensembles de données peuvent être utiles pour les domaines spécialisés, mais sans avoir une visibilite sur leurs sujets, il est difficile pour les analystes de données comme \textbf{Dan} de les trouver~\cite{Lalithsena:WI:13}.
	\item La de données qui ne cesse d'augmenter nécessite des métadonnées riches pour atteindre son vrai potentiel. Ces métadonnées permet la découverte de données, la compréhension, l'intégration et la
	maintenance. Malgré les différents modèles et les différents vocabulaires décrivant les ensembles de données des métadonnées, la capacité d'avoir un aperçu de l'ensemble de données en inspectant ses métadonnées peut être limité. 
	Par example, \textbf{Dan} a des difficultés à trouver des ensembles de données ayant une couverture géographique spécifique, car cette information n'existe pas presque dans tous les profils de jeux de données examinés.
	\item Les utilisateurs, les organisations et les gouvernements sont habilités à publier des ensembles de données. Toutefois, les administrateurs du portail de données comme \textbf{Paul} ont besoin 
	de vérifier manuellement et en permanence les portails pour détecter les spams et pour maintenir des données de haute qualité.
\end{itemize}

\subsection{Qualité de Données}

Le Web de Donnees se compose de l'information structurée soutenue par des modèles, des ontologies et des vocabulaires, et contient les liens et les points d'entrees des requestes. Cela rend l'assurance de la qualité des
 données comme un défi. Malgré le fait que la qualité de la Linked Open Data est une tendance et le sujet est d'actualite, il n'y a pas beaucoup d'efforts pour essayer de normaliser, de suivre et de formaliser les 
 des certificats ou des scores qui aideront les consommateurs de données dans leurs tâches d'intégration. Les administrateurs de portail de données comme \textbf{Paul} ont besoin d'avoir
 une vision globale de la qualité de leurs portails et que ils voulent bien intégrer ces paramètres dans les profils d'ensembles de données existants. D'autre part, les analystes de données et les 
 utilisateurs comme \textbf{Dan} veulent savoir à l'avance si l'ensemble de données qu'ils ont a certain degré de qualité pour être utilisé dans leurs rapports.

%%%%%%%%%%%%%%%%%%%%%%%%%
%%%  Thesis Contributions  %%%
%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Contributions de Thèse} \label{section:contribution}

Dans cette thèse, nous proposant un framework(une structure) permettant le provisionnement de données en libre-service pour les sources de données interne et externe dans l'entreprise. Le cadre 
contribue aux trois principaux défis décrits ci-dessus. En résumé, les principales contributions de ce travail se présentent comme suit:

\begin{adjustwidth}{-.4in}{-.4in}
	\begin{figure}[!ht]
	 \centering
	 \includegraphics[scale=0.4]{figures/architecutre_diagram.png}
	 \caption{ Schéma de l'architecture permettant le provisionnement des données en libre-service}
	 \label{fig:architecutre_diagram}
	\end{figure}
\end{adjustwidth}

\subsection{Contributions sur la Maintenance et la Découverte de l’ensemble des Données}

Concernant cet aspect de notre recherche, nous avons accompli les tâches suivantes:
\begin{itemize}
	\item Nous avons mené une enquête sur les différents modèles et vocabulaires qui décrivent les ensembles de données sur le web. Sachant que la création d'un vocabulaire commun ou d’un modèle
	constitue la clé de la communication, nous avons identifié le besoin d'un modèle harmonisé de métadonnées de l’ensemble de données. Ce modèle permettra aux consommateurs de comprendre et traiter
	plus facilement l’ensemble des données. Premièrement, nous avons mis en place un ensemble de correspondances entre les propriétés des modèles étudiés. Ceci a mené à la conception de HDL, un modèle
	harmonisé de l’ensemble de données, qui prend le meilleur sur ces modèles et l’étend afin d’assurer une couverture complète de métadonnées permettant ainsi la découverte, l'exploration et la
	réutilisation de ces données.
	\item Nous avons analysé l’ensemble des outils de profilage des ensembles de données et nous avons découvert plusieurs lacunes. En conséquence, nous avons proposé Roomba, un cadre évolutif et
	automatique pour l’extraction, la validation, la correction et la génération des descriptions des profils liés d'ensembles de données. Roomba applique plusieurs techniques afin de vérifier la
	validité des métadonnées fournies et de générer des informations descriptives et statistiques pour un ensemble de données particulier ou pour un portail entier de données.
\end{itemize}

\subsection{Contributions sur le Control de la Qualité de l’ensemble des Données}
Concernant nos contributions sur l'évaluation de la qualité des données liées, nous avons accompli les tâches suivantes:
\begin{itemize}
	\item Nous avons proposé un framework (une structure) d'évaluation de la qualité des données liées focalisant  sur les mesures objectives de l’ensemble des données. Nous avons identifié un total de 
	64 indicateurs de qualité qui ont été mappés, une fois approprié, dans quatre catégories principales (entité, ensemble de données, liens, modèles) correspondant aux principes de la publication de 
	données de base liées.
	\item Lors de l'arpentage de l’ensemble des outils de qualité de données, nous avons constaté un manque dans les outils automatiques pour vérifier les paramètres de qualité de l'ensemble de données
	proposé dans notre cadre. En conséquence, nous avons étendu Roomba  afin d’effectuer une série de contrôles de qualité des données sur les ensembles de données liés. Notre extension couvre la plupart
	des indicateurs de qualité proposés en mettant l'accent sur l'exhaustivité, l'exactitude, la provenance et les licences.
\end{itemize}

\subsection{Contributions sur l'intégration et l'enrichissement des Données}
En ce qui concerne cet aspect de notre recherche, nous avons accompli les tâches suivantes:
\begin{itemize}
	\item Nous avons créé un framework( une structure) appelé RUBIX qui permet le brassage des données potentiellement bruyants d'entreprise et des données externes. Le framework(La structure) exploite
	des bases de connaissances de référence pour annoter des données avec un ensemble de concepts sémantiques (métadonnées). Un des avantages de ces métadonnées est d'améliorer le processus d'appariement des sources de données hétérogènes au sein d'une entreprise.
	\item Les métadonnées attachées par RUBIX peuvent aussi être utilisées pour enrichir les ensembles de données existants. Toutefois, les concepts sont souvent représentés avec un grand ensemble de
	propriétés. Pour mieux recommander les plus ``importantes'' propriétés d'un concept, nous avons inversé la conception des choix faits par Google lors de la création des groupes spéciaux de graphes
	de connaissances et nous avons présenté explicitement ces choix en utilisant le vocabulaire de Fresnel, de sorte que toute application peut lire ce fichier de configuration pour décider laquelle des
	propriétés d'une entité mérite d’être enrichie.
	\item L'agrégation des actualités sociales pertinentes n'est pas une tâche facile. Nous fournissons une Interface d’Application Programmable (API), appelée SNARC,  permettant l'agrégation des actualités sociales sémantiques. Nous avons conçu un exemple d'application frontend tirant parti des capacités de SNARK pour permettre aux utilisateurs de découvrir instantanément les actualités sociales pertinentes.
\end{itemize}